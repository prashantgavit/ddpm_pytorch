{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.11.1 in /Users/prashantgavit/miniforge3/lib/python3.9/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/prashantgavit/miniforge3/lib/python3.9/site-packages (from scipy==1.11.1) (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/prashantgavit/miniforge3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Execute and Restart the kernel\n",
    "!pip install scipy==1.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiffusionForwardProcess:\n",
    "    \n",
    "    r\"\"\"\n",
    "    \n",
    "    Forward Process class as described in the \n",
    "    paper \"Denoising Diffusion Probabilistic Models\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_time_steps = 1000, \n",
    "                 beta_start = 1e-4, \n",
    "                 beta_end = 0.02\n",
    "                ):\n",
    "        \n",
    "        # Precomputing beta, alpha, and alpha_bar for all t's.\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_time_steps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        self.sqrt_alpha_bars = torch.sqrt(self.alpha_bars)\n",
    "        self.sqrt_one_minus_alpha_bars = torch.sqrt(1 - self.alpha_bars)\n",
    "        \n",
    "    def add_noise(self, original, noise, t):\n",
    "        \n",
    "        r\"\"\" Adds noise to a batch of original images at time-step t.\n",
    "        \n",
    "        :param original: Input Image Tensor\n",
    "        :param noise: Random Noise Tensor sampled from Normal Dist N(0, 1)\n",
    "        :param t: timestep of the forward process of shape -> (B, )\n",
    "        \n",
    "        Note: time-step t may differ for each image inside the batch.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        sqrt_alpha_bar_t = self.sqrt_alpha_bars.to(original.device)[t]\n",
    "        sqrt_one_minus_alpha_bar_t = self.sqrt_one_minus_alpha_bars.to(original.device)[t]\n",
    "        \n",
    "        # Broadcast to multiply with the original image.\n",
    "        sqrt_alpha_bar_t = sqrt_alpha_bar_t[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_bar_t = sqrt_one_minus_alpha_bar_t[:, None, None, None]\n",
    "        \n",
    "        # Return\n",
    "        return (sqrt_alpha_bar_t * original) \\\n",
    "                           + \\\n",
    "               (sqrt_one_minus_alpha_bar_t * noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "original = torch.randn(4, 1, 28, 28)\n",
    "noise = torch.randn(4, 1, 28, 28)\n",
    "t_steps = torch.randint(0, 1000, (4,)) \n",
    "\n",
    "# Forward Process\n",
    "dfp = DiffusionForwardProcess()\n",
    "out = dfp.add_noise(original, noise, t_steps)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiffusionReverseProcess:\n",
    "    \n",
    "    r\"\"\"\n",
    "    \n",
    "    Reverse Process class as described in the \n",
    "    paper \"Denoising Diffusion Probabilistic Models\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_time_steps = 1000, \n",
    "                 beta_start = 1e-4, \n",
    "                 beta_end = 0.02\n",
    "                ):\n",
    "        \n",
    "        # Precomputing beta, alpha, and alpha_bar for all t's.\n",
    "        self.b = torch.linspace(beta_start, beta_end, num_time_steps) # b -> beta\n",
    "        self.a = 1 - self.b # a -> alpha\n",
    "        self.a_bar = torch.cumprod(self.a, dim=0) # a_bar = alpha_bar\n",
    "        \n",
    "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
    "        \n",
    "        r\"\"\" Sample x_(t-1) given x_t and noise predicted\n",
    "             by model.\n",
    "             \n",
    "             :param xt: Image tensor at timestep t of shape -> B x C x H x W\n",
    "             :param noise_pred: Noise Predicted by model of shape -> B x C x H x W\n",
    "             :param t: Current time step\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Original Image Prediction at timestep t\n",
    "        x0 = xt - (torch.sqrt(1 - self.a_bar.to(xt.device)[t]) * noise_pred)\n",
    "        x0 = x0/torch.sqrt(self.a_bar.to(xt.device)[t])\n",
    "        x0 = torch.clamp(x0, -1., 1.) \n",
    "        \n",
    "        # mean of x_(t-1)\n",
    "        mean = (xt - ((1 - self.a.to(xt.device)[t]) * noise_pred)/(torch.sqrt(1 - self.a_bar.to(xt.device)[t])))\n",
    "        mean = mean/(torch.sqrt(self.a.to(xt.device)[t]))\n",
    "        \n",
    "        # only return mean\n",
    "        if t == 0:\n",
    "            return mean, x0\n",
    "        \n",
    "        else:\n",
    "            variance =  (1 - self.a_bar.to(xt.device)[t-1])/(1 - self.a_bar.to(xt.device)[t])\n",
    "            variance = variance * self.b.to(xt.device)[t]\n",
    "            sigma = variance**0.5\n",
    "            z = torch.randn(xt.shape).to(xt.device)\n",
    "            \n",
    "            return mean + sigma * z, x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "original = torch.randn(1, 1, 28, 28)\n",
    "noise_pred = torch.randn(1, 1, 28, 28)\n",
    "t = torch.randint(0, 1000, (1,)) \n",
    "\n",
    "# Forward Process\n",
    "drp = DiffusionReverseProcess()\n",
    "out, x0 = drp.sample_prev_timestep(original, noise_pred, t)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_time_embedding(\n",
    "    time_steps: torch.Tensor,\n",
    "    t_emb_dim: int\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    \"\"\" \n",
    "    Transform a scalar time-step into a vector representation of size t_emb_dim.\n",
    "    \n",
    "    :param time_steps: 1D tensor of size -> (Batch,)\n",
    "    :param t_emb_dim: Embedding Dimension -> for ex: 128 (scalar value)\n",
    "    \n",
    "    :return tensor of size -> (B, t_emb_dim)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert t_emb_dim%2 == 0, \"time embedding must be divisible by 2.\"\n",
    "    \n",
    "    factor = 2 * torch.arange(start = 0, \n",
    "                              end = t_emb_dim//2, \n",
    "                              dtype=torch.float32, \n",
    "                              device=time_steps.device\n",
    "                             ) / (t_emb_dim)\n",
    "    \n",
    "    factor = 10000**factor\n",
    "\n",
    "    t_emb = time_steps[:,None] # B -> (B, 1) \n",
    "    t_emb = t_emb/factor # (B, 1) -> (B, t_emb_dim//2)\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=1) # (B , t_emb_dim)\n",
    "    \n",
    "    return t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormActConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform GroupNorm, Activation, and Convolution operations.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels:int, \n",
    "                 out_channels:int, \n",
    "                 num_groups:int = 8, \n",
    "                 kernel_size: int = 3, \n",
    "                 norm:bool = True,\n",
    "                 act:bool = True\n",
    "                ):\n",
    "        super(NormActConv, self).__init__()\n",
    "        \n",
    "        # GroupNorm\n",
    "        self.g_norm = nn.GroupNorm(\n",
    "            num_groups,\n",
    "            in_channels\n",
    "        ) if norm is True else nn.Identity()\n",
    "        \n",
    "        # Activation\n",
    "        self.act = nn.SiLU() if act is True else nn.Identity()\n",
    "        \n",
    "        # Convolution\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size, \n",
    "            padding=(kernel_size - 1)//2\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.g_norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "#-----------------------------------------------------------------\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Maps the Time Embedding to the Required output Dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n_out:int, # Output Dimension\n",
    "                 t_emb_dim:int = 128 # Time Embedding Dimension\n",
    "                ):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "        \n",
    "        # Time Embedding Block\n",
    "        self.te_block = nn.Sequential(\n",
    "            nn.SiLU(), \n",
    "            nn.Linear(t_emb_dim, n_out)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.te_block(x)\n",
    "    \n",
    "#---------------------------------------------------------------\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform GroupNorm and Multiheaded Self Attention operation.    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_channels:int,\n",
    "                 num_groups:int = 8, \n",
    "                 num_heads:int = 4,\n",
    "                 norm:bool = True\n",
    "                ):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        \n",
    "        # GroupNorm\n",
    "        self.g_norm = nn.GroupNorm(\n",
    "            num_groups,\n",
    "            num_channels\n",
    "        ) if norm is True else nn.Identity()\n",
    "        \n",
    "        # Self-Attention\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            num_channels,\n",
    "            num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, channels, h, w = x.shape\n",
    "        x = x.reshape(batch_size, channels, h*w)\n",
    "        x = self.g_norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = x.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "        return x\n",
    "    \n",
    "#----------------------------------------------------------------\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform Downsampling by the factor of k across Height and Width.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels:int, \n",
    "                 out_channels:int, \n",
    "                 k:int = 2, # Downsampling factor\n",
    "                 use_conv:bool = True, # If Downsampling using conv-block\n",
    "                 use_mpool:bool = True # If Downsampling using max-pool\n",
    "                ):\n",
    "        super(Downsample, self).__init__()\n",
    "        \n",
    "        self.use_conv = use_conv\n",
    "        self.use_mpool = use_mpool\n",
    "        \n",
    "        # Downsampling using Convolution\n",
    "        self.cv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1), \n",
    "            nn.Conv2d(\n",
    "                in_channels, \n",
    "                out_channels//2 if use_mpool else out_channels, \n",
    "                kernel_size=4, \n",
    "                stride=k, \n",
    "                padding=1\n",
    "            )\n",
    "        ) if use_conv else nn.Identity()\n",
    "        \n",
    "        # Downsampling using Maxpool\n",
    "        self.mpool = nn.Sequential(\n",
    "            nn.MaxPool2d(k, k), \n",
    "            nn.Conv2d(\n",
    "                in_channels, \n",
    "                out_channels//2 if use_conv else out_channels, \n",
    "                kernel_size=1, \n",
    "                stride=1, \n",
    "                padding=0\n",
    "            )\n",
    "        ) if use_mpool else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not self.use_conv:\n",
    "            return self.mpool(x)\n",
    "        \n",
    "        if not self.use_mpool:\n",
    "            return self.cv(x)\n",
    "            \n",
    "        return torch.cat([self.cv(x), self.mpool(x)], dim=1)\n",
    "    \n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform Upsampling by the factor of k across Height and Width\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels:int, \n",
    "                 out_channels:int, \n",
    "                 k:int = 2, # Upsampling factor\n",
    "                 use_conv:bool = True, # Upsampling using conv-block\n",
    "                 use_upsample:bool = True # Upsampling using nn.upsample\n",
    "                ):\n",
    "        super(Upsample, self).__init__()\n",
    "        \n",
    "        self.use_conv = use_conv\n",
    "        self.use_upsample = use_upsample\n",
    "        \n",
    "        # Upsampling using conv\n",
    "        self.cv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels//2 if use_upsample else out_channels, \n",
    "                kernel_size=4, \n",
    "                stride=k, \n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                out_channels//2 if use_upsample else out_channels, \n",
    "                out_channels//2 if use_upsample else out_channels, \n",
    "                kernel_size = 1, \n",
    "                stride=1, \n",
    "                padding=0\n",
    "            )\n",
    "        ) if use_conv else nn.Identity()\n",
    "        \n",
    "        # Upsamling using nn.Upsample\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(\n",
    "                scale_factor=k, \n",
    "                mode = 'bilinear', \n",
    "                align_corners=False\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels//2 if use_conv else out_channels, \n",
    "                kernel_size=1, \n",
    "                stride=1, \n",
    "                padding=0\n",
    "            )\n",
    "        ) if use_upsample else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not self.use_conv:\n",
    "            return self.up(x)\n",
    "        \n",
    "        if not self.use_upsample:\n",
    "            return self.cv(x)\n",
    "        \n",
    "        return torch.cat([self.cv(x), self.up(x)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Upsample(16, 32, 2, True, True)\n",
    "x = torch.randn(4, 16, 32, 32)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownC(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform Down-convolution on the input using following approach.\n",
    "    1. Conv + TimeEmbedding\n",
    "    2. Conv\n",
    "    3. Skip-connection from input x.\n",
    "    4. Self-Attention\n",
    "    5. Skip-Connection from 3.\n",
    "    6. Downsampling\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels:int, \n",
    "                 out_channels:int, \n",
    "                 t_emb_dim:int = 128, # Time Embedding Dimension\n",
    "                 num_layers:int=2,\n",
    "                 down_sample:bool = True # True for Downsampling\n",
    "                ):\n",
    "        super(DownC, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            NormActConv(in_channels if i==0 else out_channels, \n",
    "                        out_channels\n",
    "                       ) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.conv2 = nn.ModuleList([\n",
    "            NormActConv(out_channels, \n",
    "                        out_channels\n",
    "                       ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.te_block = nn.ModuleList([\n",
    "            TimeEmbedding(out_channels, t_emb_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.attn_block = nn.ModuleList([\n",
    "            SelfAttentionBlock(out_channels) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.down_block =Downsample(out_channels, out_channels) if down_sample else nn.Identity()\n",
    "        \n",
    "        self.res_block = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_channels if i==0 else out_channels, \n",
    "                out_channels, \n",
    "                kernel_size=1\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, t_emb):\n",
    "        \n",
    "        out = x\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            resnet_input = out\n",
    "            \n",
    "            # Resnet Block\n",
    "            out = self.conv1[i](out)\n",
    "            out = out + self.te_block[i](t_emb)[:, :, None, None]\n",
    "            out = self.conv2[i](out)\n",
    "            out = out + self.res_block[i](resnet_input)\n",
    "\n",
    "            # Self Attention\n",
    "            out_attn = self.attn_block[i](out)\n",
    "            out = out + out_attn\n",
    "\n",
    "        # Downsampling\n",
    "        out = self.down_block(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidC(nn.Module):\n",
    "    \"\"\"\n",
    "    Refine the features obtained from the DownC block.\n",
    "    It refines the features using following operations:\n",
    "    \n",
    "    1. Resnet Block with Time Embedding\n",
    "    2. A Series of Self-Attention + Resnet Block with Time-Embedding \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels:int, \n",
    "                 out_channels:int,\n",
    "                 t_emb_dim:int = 128,\n",
    "                 num_layers:int = 2\n",
    "                ):\n",
    "        super(MidC, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            NormActConv(in_channels if i==0 else out_channels, \n",
    "                        out_channels\n",
    "                       ) for i in range(num_layers + 1)\n",
    "        ])\n",
    "        \n",
    "        self.conv2 = nn.ModuleList([\n",
    "            NormActConv(out_channels, \n",
    "                        out_channels\n",
    "                       ) for _ in range(num_layers + 1)\n",
    "        ])\n",
    "        \n",
    "        self.te_block = nn.ModuleList([\n",
    "            TimeEmbedding(out_channels, t_emb_dim) for _ in range(num_layers + 1)\n",
    "        ])\n",
    "        \n",
    "        self.attn_block = nn.ModuleList([\n",
    "            SelfAttentionBlock(out_channels) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.res_block = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_channels if i==0 else out_channels, \n",
    "                out_channels, \n",
    "                kernel_size=1\n",
    "            ) for i in range(num_layers + 1)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, t_emb):\n",
    "        out = x\n",
    "        \n",
    "        # First-Resnet Block\n",
    "        resnet_input = out\n",
    "        out = self.conv1[0](out)\n",
    "        out = out + self.te_block[0](t_emb)[:, :, None, None]\n",
    "        out = self.conv2[0](out)\n",
    "        out = out + self.res_block[0](resnet_input)\n",
    "        \n",
    "        # Sequence of Self-Attention + Resnet Blocks\n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            # Self Attention\n",
    "            out_attn = self.attn_block[i](out)\n",
    "            out = out + out_attn\n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.conv1[i+1](out)\n",
    "            out = out + self.te_block[i+1](t_emb)[:, :, None, None]\n",
    "            out = self.conv2[i+1](out)\n",
    "            out = out + self.res_block[i+1](resnet_input)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpC(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform Up-convolution on the input using following approach.\n",
    "    1. Upsampling\n",
    "    2. Conv + TimeEmbedding\n",
    "    3. Conv\n",
    "    4. Skip-connection from 1.\n",
    "    5. Self-Attention\n",
    "    6. Skip-Connection from 3.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels:int, \n",
    "                 out_channels:int, \n",
    "                 t_emb_dim:int = 128, # Time Embedding Dimension\n",
    "                 num_layers:int = 2,\n",
    "                 up_sample:bool = True # True for Upsampling\n",
    "                ):\n",
    "        super(UpC, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.conv1 = nn.ModuleList([\n",
    "            NormActConv(in_channels if i==0 else out_channels, \n",
    "                        out_channels\n",
    "                       ) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.conv2 = nn.ModuleList([\n",
    "            NormActConv(out_channels, \n",
    "                        out_channels\n",
    "                       ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.te_block = nn.ModuleList([\n",
    "            TimeEmbedding(out_channels, t_emb_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.attn_block = nn.ModuleList([\n",
    "            SelfAttentionBlock(out_channels) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.up_block =Upsample(in_channels, in_channels//2) if up_sample else nn.Identity()\n",
    "        \n",
    "        self.res_block = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                in_channels if i==0 else out_channels, \n",
    "                out_channels, \n",
    "                kernel_size=1\n",
    "            ) for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, down_out, t_emb):\n",
    "        \n",
    "        # Upsampling\n",
    "        x = self.up_block(x)\n",
    "        x = torch.cat([x, down_out], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            resnet_input = out\n",
    "            \n",
    "            # Resnet Block\n",
    "            out = self.conv1[i](out)\n",
    "            out = out + self.te_block[i](t_emb)[:, :, None, None]\n",
    "            out = self.conv2[i](out)\n",
    "            out = out + self.res_block[i](resnet_input)\n",
    "\n",
    "            # Self Attention\n",
    "            out_attn = self.attn_block[i](out)\n",
    "            out = out + out_attn\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-net architecture which is used to predict noise\n",
    "    in the paper \"Denoising Diffusion Probabilistic Model\".\n",
    "    \n",
    "    U-net consists of Series of DownC blocks followed by MidC\n",
    "    followed by UpC.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 im_channels: int = 1, # RGB \n",
    "                 down_ch: list = [32, 64, 128, 256],\n",
    "                 mid_ch: list = [256, 256, 128],\n",
    "                 up_ch: list[int] = [256, 128, 64, 16],\n",
    "                 down_sample: list[bool] = [True, True, False],\n",
    "                 t_emb_dim: int = 128,\n",
    "                 num_downc_layers:int = 2, \n",
    "                 num_midc_layers:int = 2, \n",
    "                 num_upc_layers:int = 2\n",
    "                ):\n",
    "        super(Unet, self).__init__()\n",
    "        \n",
    "        self.im_channels = im_channels\n",
    "        self.down_ch = down_ch\n",
    "        self.mid_ch = mid_ch\n",
    "        self.up_ch = up_ch\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.down_sample = down_sample\n",
    "        self.num_downc_layers = num_downc_layers\n",
    "        self.num_midc_layers = num_midc_layers\n",
    "        self.num_upc_layers = num_upc_layers\n",
    "        \n",
    "        self.up_sample = list(reversed(self.down_sample)) # [False, True, True]\n",
    "        \n",
    "        # Initial Convolution\n",
    "        self.cv1 = nn.Conv2d(self.im_channels, self.down_ch[0], kernel_size=3, padding=1)\n",
    "        \n",
    "        # Initial Time Embedding Projection\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim), \n",
    "            nn.SiLU(), \n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # DownC Blocks\n",
    "        self.downs = nn.ModuleList([\n",
    "            DownC(\n",
    "                self.down_ch[i], \n",
    "                self.down_ch[i+1], \n",
    "                self.t_emb_dim, \n",
    "                self.num_downc_layers, \n",
    "                self.down_sample[i]\n",
    "            ) for i in range(len(self.down_ch) - 1)\n",
    "        ])\n",
    "        \n",
    "        # MidC Block\n",
    "        self.mids = nn.ModuleList([\n",
    "            MidC(\n",
    "                self.mid_ch[i], \n",
    "                self.mid_ch[i+1], \n",
    "                self.t_emb_dim, \n",
    "                self.num_midc_layers\n",
    "            ) for i in range(len(self.mid_ch) - 1)\n",
    "        ])\n",
    "        \n",
    "        # UpC Block\n",
    "        self.ups = nn.ModuleList([\n",
    "            UpC(\n",
    "                self.up_ch[i], \n",
    "                self.up_ch[i+1], \n",
    "                self.t_emb_dim, \n",
    "                self.num_upc_layers, \n",
    "                self.up_sample[i]\n",
    "            ) for i in range(len(self.up_ch) - 1)\n",
    "        ])\n",
    "        \n",
    "        # Final Convolution\n",
    "        self.cv2 = nn.Sequential(\n",
    "            nn.GroupNorm(8, self.up_ch[-1]), \n",
    "            nn.Conv2d(self.up_ch[-1], self.im_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        \n",
    "        out = self.cv1(x)\n",
    "        \n",
    "        # Time Projection\n",
    "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        # DownC outputs\n",
    "        down_outs = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        \n",
    "        # MidC outputs\n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        \n",
    "        # UpC Blocks\n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            \n",
    "        # Final Conv\n",
    "        out = self.cv2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 32, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "model = Unet()\n",
    "x = torch.randn(4, 1, 32, 32)\n",
    "t = torch.randint(0, 10, (4,))\n",
    "model(x, t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class CustomMnistDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Reads the MNIST data from csv file given file path.\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_path, num_datapoints = None):\n",
    "        super(CustomMnistDataset, self).__init__()\n",
    "        \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Will be useful later while evaluating\n",
    "        if num_datapoints is not None:\n",
    "            self.df = self.df.iloc[0:num_datapoints]\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def  __getitem__(self, index):\n",
    "        # Read\n",
    "        img = self.df.iloc[index].filter(regex='pixel').values\n",
    "        img =  np.reshape(img, (28, 28)).astype(np.uint8)\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        img_tensor = torchvision.transforms.ToTensor()(img) # [0, 1]\n",
    "        img_tensor = 2*img_tensor - 1 # [-1, 1]\n",
    "        \n",
    "        return img_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    model_path = 'ddpm_unet.pth'\n",
    "    train_csv_path = '/kaggle/input/digit-recognizer/train.csv'\n",
    "    test_csv_path = '/kaggle/input/digit-recognizer/test.csv'\n",
    "    generated_csv_path = 'mnist_generated_data.csv'\n",
    "    num_epochs = 50\n",
    "    lr = 1e-4\n",
    "    num_timesteps = 1000\n",
    "    batch_size = 128\n",
    "    img_size = 28\n",
    "    in_channels = 1\n",
    "    num_img_to_generate = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(cfg):\n",
    "    \n",
    "    # Dataset and Dataloader\n",
    "    mnist_ds = CustomMnistDataset(cfg.train_csv_path)\n",
    "    mnist_dl = DataLoader(mnist_ds, cfg.batch_size, shuffle=True)\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f'Device: {device}\\n')\n",
    "    \n",
    "    # Initiate Model\n",
    "    model = Unet().to(device)\n",
    "    \n",
    "    # Initialize Optimizer and Loss Function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    # Diffusion Forward Process to add noise\n",
    "    dfp = DiffusionForwardProcess()\n",
    "    \n",
    "    # Best Loss\n",
    "    best_eval_loss = float('inf')\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(cfg.num_epochs):\n",
    "        \n",
    "        # For Loss Tracking\n",
    "        losses = []\n",
    "        \n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loop over dataloader\n",
    "        for imgs in tqdm(mnist_dl):\n",
    "            \n",
    "            imgs = imgs.to(device)\n",
    "            \n",
    "            # Generate noise and timestamps\n",
    "            noise = torch.randn_like(imgs).to(device)\n",
    "            t = torch.randint(0, cfg.num_timesteps, (imgs.shape[0],)).to(device)\n",
    "            \n",
    "            # Add noise to the images using Forward Process\n",
    "            noisy_imgs = dfp.add_noise(imgs, noise, t)\n",
    "            \n",
    "            # Avoid Gradient Accumulation\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Predict noise using U-net Model\n",
    "            noise_pred = model(noisy_imgs, t)\n",
    "            \n",
    "            # Calculate Loss\n",
    "            loss = criterion(noise_pred, noise)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Backprop + Update model params\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Mean Loss\n",
    "        mean_epoch_loss = np.mean(losses)\n",
    "        \n",
    "        # Display\n",
    "        print('Epoch:{} | Loss : {:.4f}'.format(\n",
    "            epoch + 1,\n",
    "            mean_epoch_loss,\n",
    "        ))\n",
    "        \n",
    "        # Save based on train-loss\n",
    "        if mean_epoch_loss < best_eval_loss:\n",
    "            best_eval_loss = mean_epoch_loss\n",
    "            torch.save(model, cfg.model_path)\n",
    "            \n",
    "    print(f'Done training.....')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/digit-recognizer/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m9/q589_lfd183gkt5cp7dkr6c80000gn/T/ipykernel_17842/3643742195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/m9/q589_lfd183gkt5cp7dkr6c80000gn/T/ipykernel_17842/2850914626.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Dataset and Dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmnist_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomMnistDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_csv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmnist_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/m9/q589_lfd183gkt5cp7dkr6c80000gn/T/ipykernel_17842/3107069509.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_path, num_datapoints)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomMnistDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Will be useful later while evaluating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/digit-recognizer/train.csv'"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "cfg = CONFIG()\n",
    "\n",
    "# TRAIN\n",
    "train(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(cfg):\n",
    "    \"\"\"\n",
    "    Given Pretrained DDPM U-net model, Generate Real-life\n",
    "    Images from noise by going backward step by step. i.e.,\n",
    "    Mapping of Random Noise to Real-life images.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #print(f'Device: {device}\\n')\n",
    "    \n",
    "    # Initialize Diffusion Reverse Process\n",
    "    drp = DiffusionReverseProcess()\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    model = torch.load(cfg.model_path).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate Noise sample from N(0, 1)\n",
    "    xt = torch.randn(1, cfg.in_channels, cfg.img_size, cfg.img_size).to(device)\n",
    "    \n",
    "    # Denoise step by step by going backward.\n",
    "    with torch.no_grad():\n",
    "        for t in reversed(range(cfg.num_timesteps)):\n",
    "            noise_pred = model(xt, torch.as_tensor(t).unsqueeze(0).to(device))\n",
    "            xt, x0 = drp.sample_prev_timestep(xt, noise_pred, torch.as_tensor(t).to(device))\n",
    "\n",
    "    # Convert the image to proper scale\n",
    "    xt = torch.clamp(xt, -1., 1.).detach().cpu()\n",
    "    xt = (xt + 1) / 2\n",
    "    \n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and config\n",
    "cfg = CONFIG()\n",
    "\n",
    "# Generate\n",
    "generated_imgs = []\n",
    "for i in tqdm(range(cfg.num_img_to_generate)):\n",
    "    xt = generate(cfg)\n",
    "    xt = 255 * xt[0][0].numpy()\n",
    "    generated_imgs.append(xt.astype(np.uint8).flatten())\n",
    "\n",
    "# Save Generated Data CSV\n",
    "generated_df = pd.DataFrame(generated_imgs, columns=[f'pixel{i}' for i in range(784)])\n",
    "generated_df.to_csv(cfg.generated_csv_path, index=False)\n",
    "\n",
    "# Visualize\n",
    "from matplotlib import pyplot as plt\n",
    "fig, axes = plt.subplots(8, 8, figsize=(5, 5))\n",
    "\n",
    "# Plot each image in the corresponding subplot\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(np.reshape(generated_imgs[i], (28, 28)), cmap='gray')  # You might need to adjust the colormap based on your images\n",
    "    ax.axis('off')  # Turn off axis labels\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(dataloader, \n",
    "                   model, \n",
    "                   preprocess, # Preprocessing Transform for InceptionV3\n",
    "                   device = 'cpu'\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Given Dataloader and Model, Generate N X 2048\n",
    "    Dimensional activation map for N data points\n",
    "    in dataloader.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to evaluation Mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Save activations\n",
    "    pred_arr = np.zeros((len(dataloader.dataset), 2048))\n",
    "    \n",
    "    # Batch Size\n",
    "    batch_size = dataloader.batch_size\n",
    "    \n",
    "    # Loop over Dataloader\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(dataloader)):\n",
    "            \n",
    "            # Transform the Batch according to Inceptionv3 specification\n",
    "            batch = torch.stack([preprocess(img) for img in batch]).to(device)\n",
    "            \n",
    "            # Predict\n",
    "            pred = model(batch).cpu().numpy()\n",
    "            \n",
    "            # Store\n",
    "            pred_arr[i*batch_size : i*batch_size + batch.size(0), :] = pred\n",
    "            \n",
    "    return pred_arr\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "\n",
    "def calculate_activation_statistics(dataloader, \n",
    "                                    model, \n",
    "                                    preprocess, \n",
    "                                    device='cpu'\n",
    "                                   ):\n",
    "    \"\"\"\n",
    "    Get mean vector and covariance matrix of the activation maps.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get activation maps\n",
    "    act = get_activation(dataloader, \n",
    "                         model, \n",
    "                         preprocess, # Preprocessing Transform for InceptionV3\n",
    "                         device\n",
    "                       )\n",
    "    # Mean\n",
    "    mu = np.mean(act, axis=0)\n",
    "    \n",
    "    # Covariance Metric\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "from scipy import linalg\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given Mean and Sigma of Real and Generated Data,\n",
    "    it calculates FID between them using:\n",
    "     \n",
    "     d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "     \n",
    "    \"\"\"\n",
    "    # Make sure they have appropriate dims\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "    \n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    \n",
    "    # Handle various cases\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = (\n",
    "            \"fid calculation produces singular product; \"\n",
    "            \"adding %s to diagonal of cov estimates\"\n",
    "        ) % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # Numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform to Convert Output of CustomMnistDataset class to Inception format.\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_inception = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: (x + 1.0)/2.0), # [-1, 1] => [0, 1]\n",
    "    transforms.ToPILImage(), # Tensor to PIL Image \n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to RGB format\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "\n",
    "])\n",
    "\n",
    "# Load InceptionV3 Model\n",
    "import torchvision.models as models\n",
    "from torchvision.models.inception import Inception_V3_Weights\n",
    "model = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Identity()\n",
    "\n",
    "# Mean and Sigma For Generated Data\n",
    "mnist_ds = CustomMnistDataset(cfg.generated_csv_path, cfg.num_img_to_generate)\n",
    "mnist_dl = DataLoader(mnist_ds, cfg.batch_size//4, shuffle=False)\n",
    "mu1, sigma1 = calculate_activation_statistics(mnist_dl, model, preprocess = transform_inception, device='cuda')\n",
    "\n",
    "# Mean and Sigma for Test Data\n",
    "mnist_ds = CustomMnistDataset(cfg.test_csv_path, cfg.num_img_to_generate)\n",
    "mnist_dl = DataLoader(mnist_ds, cfg.batch_size//4, shuffle=False)\n",
    "mu2, sigma2 = calculate_activation_statistics(mnist_dl, model, preprocess = transform_inception, device='cuda')\n",
    "\n",
    "# Calculate FID\n",
    "fid = calculate_frechet_distance(mu1, sigma1, mu2, sigma2)\n",
    "print(f'FID-Score: {fid}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    },
    {
     "datasetId": 4942657,
     "sourceId": 8576869,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
